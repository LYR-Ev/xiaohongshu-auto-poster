"""
AIæ–‡æ¡ˆç”Ÿæˆæ¨¡å—
æ”¯æŒæœ¬åœ° Ollama / Anthropic ç­‰ç”Ÿæˆè®°å•è¯æ–‡æ¡ˆ
"""
import os
import re
from typing import Optional, Dict, Any
from dotenv import load_dotenv

# Anthropic ä½œä¸ºå¯é€‰è½¯ä¾èµ–ï¼šæ²¡å®‰è£…æˆ–æ²¡é… key æ—¶è‡ªåŠ¨é€€å›æœ¬åœ° Ollama
try:
    from anthropic import Anthropic  # type: ignore
except ImportError:  # æœªå®‰è£… anthropic æ—¶ä¸æŠ¥é”™
    Anthropic = None  # type: ignore

from llm_client import generate_text
from prompts.word_learning import build_word_learning_prompt, PROMPT_VERSION
from structured_parser import StructuredPostParser

load_dotenv()

# ç»“æ„åŒ–è¾“å‡ºä¸­çš„æ®µè½æ ‡è®°ï¼Œç”¨äºè§£æï¼ˆåŒ…å«ã€metaã€‘ï¼‰
_STRUCTURED_SECTIONS = ("ã€æ ‡é¢˜ã€‘", "ã€å•è¯å¡ã€‘", "ã€é…å›¾å»ºè®®ã€‘", "ã€æ­£æ–‡ã€‘", "ã€æ ‡ç­¾ã€‘", "ã€metaã€‘")


class WordLearningParser(StructuredPostParser):
    """å•è¯å­¦ä¹ å¸–è§£æå™¨ï¼ˆç»§æ‰¿åŸºç±»ï¼‰"""
    
    sections = ("ã€æ ‡é¢˜ã€‘", "ã€å•è¯å¡ã€‘", "ã€é…å›¾å»ºè®®ã€‘", "ã€æ­£æ–‡ã€‘", "ã€æ ‡ç­¾ã€‘", "ã€metaã€‘")
    
    def _post_process(self, sections: Dict[str, str], word: str) -> Dict[str, Any]:
        """å°†è§£æå‡ºçš„æ®µè½è½¬æ¢ä¸ºä¸šåŠ¡æ ¼å¼"""
        title = sections.get("ã€æ ‡é¢˜ã€‘", "").strip() or f"ğŸ“š ä»Šå¤©å­¦å•è¯ï¼š{word}"
        å•è¯å¡ = sections.get("ã€å•è¯å¡ã€‘", "").strip()
        é…å›¾å»ºè®® = sections.get("ã€é…å›¾å»ºè®®ã€‘", "").strip()
        æ­£æ–‡ = sections.get("ã€æ­£æ–‡ã€‘", "").strip()
        æ ‡ç­¾_raw = sections.get("ã€æ ‡ç­¾ã€‘", "").strip()
        meta_raw = sections.get("ã€metaã€‘", "").strip()
        
        tags = self.extract_tags(æ ‡ç­¾_raw)
        if not tags:
            tags = ["è‹±è¯­å­¦ä¹ ", "è®°å•è¯", "è‹±è¯­è¯æ±‡", "å­¦ä¹ æ‰“å¡", "è‹±è¯­å¹²è´§"]
        
        content = (å•è¯å¡ + "\n\n" + æ­£æ–‡).strip() if å•è¯å¡ else æ­£æ–‡
        
        meta = self.extract_meta(meta_raw)
        
        return {
            "word": word,
            "title": title,
            "content": content,
            "tags": tags[:8],
            "image_suggestion": é…å›¾å»ºè®® or None,
            "meta": meta,  # åŒ…å« prompt version ç­‰ä¿¡æ¯
        }


class ContentGenerator:
    """AIæ–‡æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self):
        # ä»…ä¿ç•™ Anthropic ä½œä¸ºå¯é€‰è¿œç¨‹å¤‡ä»½ï¼Œæœ¬åœ°é»˜è®¤èµ° Ollama
        self.anthropic_client = None
        self.word_parser = WordLearningParser()  # ä½¿ç”¨è§£æå™¨å®ä¾‹

        # åˆå§‹åŒ– Anthropic å®¢æˆ·ç«¯ï¼ˆè½¯ä¾èµ–ï¼šæ—¢è¦è£…äº†åŒ…ï¼Œåˆè¦é…äº† key æ‰ä¼šå¯ç”¨ï¼‰
        anthropic_key = os.getenv("ANTHROPIC_API_KEY")
        if Anthropic and anthropic_key and anthropic_key != "your_anthropic_api_key_here":  # type: ignore
            self.anthropic_client = Anthropic(api_key=anthropic_key)  # type: ignore
    
    def generate_word_post(self, word: str, level: str = "CET-4") -> str:
        """
        ä½¿ç”¨ç‹¬ç«‹ Prompt æ¨¡æ¿ç”Ÿæˆè‹±è¯­å•è¯å­¦ä¹ å¸–æ–‡æ¡ˆã€‚
        å†…éƒ¨è°ƒç”¨ llm_client.generate_textï¼Œä¸å†™æ­» Promptã€‚

        Args:
            word: è¦å­¦ä¹ çš„è‹±è¯­å•è¯
            level: éš¾åº¦æ°´å¹³ï¼Œé»˜è®¤ "CET-4"

        Returns:
            LLM ç”Ÿæˆçš„æ–‡æ¡ˆåŸæ–‡ï¼ˆå­—ç¬¦ä¸²ï¼‰
        """
        prompt = build_word_learning_prompt(word=word, level=level)
        return generate_text(prompt)

    def parse_structured_word_post(self, text: str, word: str) -> dict:
        """
        è§£æã€Œå…­æ®µå¼ã€ç»“æ„åŒ–è¾“å‡ºï¼ˆå«ã€metaã€‘ï¼‰ï¼Œå¾—åˆ°æ ‡é¢˜ã€æ­£æ–‡ã€æ ‡ç­¾ã€é…å›¾å»ºè®®ã€‚
        ä½¿ç”¨ StructuredPostParser åŸºç±»ï¼Œä¾¿äºæ‰©å±• phrase / grammar ç­‰ç±»å‹ã€‚

        Args:
            text: LLM æŒ‰ã€æ ‡é¢˜ã€‘ã€å•è¯å¡ã€‘ã€é…å›¾å»ºè®®ã€‘ã€æ­£æ–‡ã€‘ã€æ ‡ç­¾ã€‘ã€metaã€‘è¾“å‡ºçš„åŸæ–‡
            word: å½“å‰å•è¯ï¼ˆç”¨äºå…œåº•æ ‡é¢˜ç­‰ï¼‰

        Returns:
            dict: word, title, content, tags, image_suggestion, meta
        """
        return self.word_parser.parse(text, word=word)

    def generate_word_content(self, word: Optional[str] = None, theme: Optional[str] = None) -> dict:
        """
        ç”Ÿæˆè®°å•è¯æ–‡æ¡ˆ
        
        Args:
            word: æŒ‡å®šå•è¯ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™éšæœºé€‰æ‹©ï¼‰
            theme: ä¸»é¢˜ï¼ˆå¯é€‰ï¼Œå¦‚"æ—¥å¸¸ç”¨è¯­"ã€"å•†åŠ¡è‹±è¯­"ç­‰ï¼‰
        
        Returns:
            åŒ…å«æ ‡é¢˜ã€æ­£æ–‡ã€æ ‡ç­¾çš„å­—å…¸
        """
        # å¦‚æœæ²¡æœ‰æŒ‡å®šå•è¯ï¼Œç”Ÿæˆä¸€ä¸ªå¸¸è§å•è¯
        if not word:
            word = self._get_random_word(theme)
        
        # ç”Ÿæˆæ–‡æ¡ˆæç¤ºè¯
        prompt = self._build_prompt(word, theme)

        # ä¼˜å…ˆä½¿ç”¨ Claudeï¼›å¦åˆ™å›è½åˆ°æœ¬åœ° Ollamaï¼ˆllm_clientï¼‰
        if self.anthropic_client:
            content = self._generate_with_claude(prompt)
        else:
            content = generate_text(prompt)
        
        # è§£æç”Ÿæˆçš„å†…å®¹
        return self._parse_content(content, word)
    
    def _get_random_word(self, theme: Optional[str] = None) -> str:
        """è·å–éšæœºå•è¯"""
        # è¿™é‡Œå¯ä»¥è¿æ¥å•è¯æ•°æ®åº“
        import random
        from pathlib import Path

        level = theme or "cet4"  # é»˜è®¤ CET4

        file_map = {
            "cet4": "data/cet4.txt",
            "cet6": "data/cet6.txt",
           
        }

        word_file = file_map.get(level)
        if not word_file:
            raise ValueError(f"Unsupported word level: {level}")
        
        path = Path(word_file)
        if not word_file:
            raise ValueError(f"Unsupported word level: {level}")

        path = Path(word_file)
        if not path.exists():
            raise FileNotFoundError(f"Word list not found: {path}")

        with path.open("r", encoding="utf-8") as f:
            words = [line.strip() for line in f if line.strip()]

        if not words:
            raise RuntimeError(f"Word list is empty: {path}")

        return random.choice(words)
    
    def _build_prompt(self, word: str, theme: Optional[str] = None) -> str:
        """æ„å»ºAIæç¤ºè¯"""
        theme_text = f"ä¸»é¢˜ï¼š{theme}ï¼Œ" if theme else ""
        return f"""è¯·ä¸ºå°çº¢ä¹¦å¹³å°ç”Ÿæˆä¸€ç¯‡å…³äºè‹±è¯­å•è¯"{word}"çš„è®°å•è¯æ–‡æ¡ˆã€‚

è¦æ±‚ï¼š
1. æ ‡é¢˜è¦å¸å¼•çœ¼çƒï¼Œä½¿ç”¨emojiè¡¨æƒ…ç¬¦å·ï¼Œé•¿åº¦15-25å­—
2. æ­£æ–‡è¦ç”ŸåŠ¨æœ‰è¶£ï¼ŒåŒ…å«ï¼š
   - å•è¯çš„å‘éŸ³ï¼ˆéŸ³æ ‡ï¼‰
   - ä¸­æ–‡é‡Šä¹‰
   - è®°å¿†æŠ€å·§ï¼ˆå¯ä»¥æ˜¯è”æƒ³ã€è¯æ ¹è¯ç¼€ã€æ•…äº‹ç­‰ï¼‰
   - å®ç”¨ä¾‹å¥ï¼ˆä¸­è‹±æ–‡å¯¹ç…§ï¼Œ2-3ä¸ªï¼‰
   - ç›¸å…³è¯æ±‡æ‰©å±•
3. ä½¿ç”¨å°çº¢ä¹¦é£æ ¼ï¼šè½»æ¾æ´»æ³¼ã€æœ‰äº’åŠ¨æ„Ÿã€ä½¿ç”¨emoji
4. æ·»åŠ 5-8ä¸ªç›¸å…³è¯é¢˜æ ‡ç­¾ï¼ˆæ ¼å¼ï¼š#è¯é¢˜#ï¼‰
5. æ–‡æ¡ˆæ€»é•¿åº¦æ§åˆ¶åœ¨300-500å­—

{theme_text}è¯·ç¡®ä¿å†…å®¹å‡†ç¡®ä¸”æœ‰è¶£ï¼Œèƒ½å¤Ÿå¸®åŠ©è¯»è€…è½»æ¾è®°ä½è¿™ä¸ªå•è¯ã€‚

è¯·ç›´æ¥è¾“å‡ºæ–‡æ¡ˆå†…å®¹ï¼Œä¸éœ€è¦é¢å¤–è¯´æ˜ã€‚"""
    
    def _generate_with_openai(self, prompt: str) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆå†…å®¹"""
        response = self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å°çº¢ä¹¦å†…å®¹åˆ›ä½œä¸“å®¶ï¼Œæ“…é•¿åˆ›ä½œæœ‰è¶£ã€å®ç”¨çš„è‹±è¯­å­¦ä¹ å†…å®¹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8,
            max_tokens=1000
        )
        return response.choices[0].message.content
    
    def _generate_with_claude(self, prompt: str) -> str:
        """ä½¿ç”¨Claudeç”Ÿæˆå†…å®¹"""
        response = self.anthropic_client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=1000,
            temperature=0.8,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        return response.content[0].text
    
    def _parse_content(self, content: str, word: str) -> dict:
        """è§£æç”Ÿæˆçš„å†…å®¹"""
        lines = content.strip().split('\n')
        
        # æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€è¡Œï¼‰
        title = lines[0].strip() if lines else f"ğŸ“š ä»Šå¤©å­¦å•è¯ï¼š{word}"
        
        # æå–æ­£æ–‡ï¼ˆå»é™¤æ ‡é¢˜åçš„å†…å®¹ï¼‰
        body_lines = [line.strip() for line in lines[1:] if line.strip()]
        body = '\n\n'.join(body_lines)
        
        # æå–æ ‡ç­¾ï¼ˆä»¥#å¼€å¤´çš„å†…å®¹ï¼‰
        tags = []
        for line in body_lines:
            if '#' in line:
                import re
                found_tags = re.findall(r'#([^#]+)#', line)
                tags.extend(found_tags)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾ï¼Œæ·»åŠ é»˜è®¤æ ‡ç­¾
        if not tags:
            tags = ["è‹±è¯­å­¦ä¹ ", "è®°å•è¯", "è‹±è¯­è¯æ±‡", "å­¦ä¹ æ‰“å¡", "è‹±è¯­å¹²è´§"]
        
        return {
            "word": word,
            "title": title,
            "content": body,
            "tags": tags[:8],  # é™åˆ¶æœ€å¤š8ä¸ªæ ‡ç­¾
            "full_text": content
        }
