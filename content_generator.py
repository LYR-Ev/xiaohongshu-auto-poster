"""
AIæ–‡æ¡ˆç”Ÿæˆæ¨¡å—
æ”¯æŒæœ¬åœ° Ollama ç”Ÿæˆè®°å•è¯æ–‡æ¡ˆï¼Œå”¯ä¸€ Prompt å…¥å£ä¸º prompts.word_learning.build_word_learning_promptã€‚
"""
import re
import random
from pathlib import Path
from typing import Optional, Dict, Any, List, Callable
from dotenv import load_dotenv

from llm_client import generate_text
from prompts.word_learning import build_word_learning_prompt, PROMPT_VERSION
from structured_parser import StructuredPostParser
from renderers.xhs_word_renderer import render_xhs_word_post, WordPost

load_dotenv()

# level -> å•è¯åº“æ–‡ä»¶è·¯å¾„ï¼ˆä¸€è¡Œä¸€ä¸ªå•è¯ï¼‰
LEVEL_WORD_FILES: Dict[str, str] = {
    "CET-4": "data/CET4.txt",
    "CET-6": "data/CET6.txt",
    "è€ƒç ”": "data/è€ƒç ”.txt",
    "CET4": "data/CET4.txt",
    "CET6": "data/CET6.txt",
    "cet-4": "data/CET4.txt",
    "cet-6": "data/CET6.txt",
    "cet4": "data/CET4.txt",
    "cet6": "data/CET6.txt",
}


class AllWordsUsedError(Exception):
    """è¯¥ level ä¸‹å•è¯åº“ä¸­çš„è¯å‡å·²å‡ºç°åœ¨ posts è¡¨ä¸­ï¼Œæ— æ³•å†é€‰æœªå‘è¯ã€‚"""
    def __init__(self, level: str):
        self.level = level
        super().__init__(f"è¯¥çº§åˆ«å•è¯å·²å…¨éƒ¨ä½¿ç”¨å®Œæ¯•: {level}ï¼ˆword + level + prompt_version å‡åœ¨ posts ä¸­å·²æœ‰è®°å½•ï¼‰")

# åˆ†æ®µå…œåº•æ­£åˆ™ï¼šä»»æ„å‘½ä¸­å³è§†ä¸ºè¯¥æ®µå¼€å§‹ï¼Œé¿å…æ¨¡å‹æ¢è¯´æ³•ï¼ˆä¾‹å¥/Examples/å®ç”¨ä¾‹å­ï¼‰å¯¼è‡´æ‹†æ®µå¤±è´¥
EXAMPLE_SPLIT_PATTERNS = [
    r"å®ç”¨ä¾‹å¥\s*[ï¼š:]",
    r"ä¾‹å¥\s*[ï¼š:]",
    r"Examples?\s*[ï¼š:]",
    r"å®ç”¨ä¾‹å­\s*[ï¼š:]",
    r"ã€ä¾‹å¥ã€‘",
]
RELATED_SPLIT_PATTERNS = [
    r"ç›¸å…³è¯æ±‡æ‰©å±•\s*[ï¼š:]",
    r"ç›¸å…³è¯æ±‡\s*[ï¼š:]",
    r"æ‰©å±•\s*[ï¼š:]",
    r"Related\s*[ï¼š:]",
    r"ã€ç›¸å…³è¯æ±‡ã€‘",
]


def _find_first_match(text: str, patterns: List[str]) -> tuple:
    """è¿”å› (ä½ç½®, åŒ¹é…åˆ°çš„æ­£åˆ™åœ¨ text ä¸­çš„ç»“æŸä½ç½®)ã€‚æœªå‘½ä¸­è¿”å› (-1, -1)ã€‚"""
    for pat in patterns:
        m = re.search(pat, text, re.IGNORECASE)
        if m:
            return m.start(), m.end()
    return -1, -1


def _parse_body_into_word_post(æ­£æ–‡: str) -> tuple:
    """
    ä»ã€æ­£æ–‡ã€‘ä¸­æ‹†å‡º memory_storyã€examplesã€relatedã€‚
    åˆ†æ®µç”¨å…œåº•æ­£åˆ™ï¼Œä»»æ„å‘½ä¸­å³è§†ä¸ºä¾‹å¥/æ‰©å±•æ®µå¼€å§‹ï¼›memory_story åªå–ç¬¬ä¸€ä¸ªç©ºè¡Œå‰çš„è‡ªç„¶æ®µã€‚
    """
    memory_story = ""
    examples: List[str] = []
    related: List[str] = []
    if not æ­£æ–‡ or not æ­£æ–‡.strip():
        return memory_story, examples, related

    text = æ­£æ–‡.strip()
    # ä¾‹å¥æ®µï¼šç”¨æ­£åˆ™æ‰¾ä»»æ„åˆ†ç•Œ
    ex_pos, ex_end = _find_first_match(text, EXAMPLE_SPLIT_PATTERNS)
    if ex_pos == -1:
        # memory_story åªå–ã€Œç¬¬ä¸€ä¸ªç©ºè¡Œä¹‹å‰ã€çš„å®Œæ•´æ•…äº‹å—ï¼Œé¿å…é‡Šä¹‰é‡å¤ã€æ•…äº‹è¢«æˆªæ–­
        memory_story = text.split("\n\n")[0].strip()
        return memory_story, examples, related

    before_ex = text[:ex_pos].strip()
    memory_story = before_ex.split("\n\n")[0].strip()
    rest = text[ex_end:].strip()

    # æ‰©å±•æ®µï¼šç”¨æ­£åˆ™æ‰¾ä»»æ„åˆ†ç•Œ
    rel_pos, rel_end = _find_first_match(rest, RELATED_SPLIT_PATTERNS)
    if rel_pos >= 0:
        examples_block = rest[:rel_pos].strip()
        related_block = rest[rel_end:].strip()
        for line in related_block.split("\n"):
            line = line.strip()
            if line.startswith("-") or line.startswith("â€¢"):
                line = line.lstrip("-â€¢").strip()
                if line:
                    related.append(line)
    else:
        examples_block = rest

    for line in examples_block.split("\n"):
        line = line.strip()
        if not line:
            continue
        if line.startswith("-") or line.startswith("â€¢"):
            examples.append(line.lstrip("-â€¢").strip())
        elif re.match(r"^\d+[\.ï¼]\s*", line):
            examples.append(re.sub(r"^\d+[\.ï¼]\s*", "", line))
        elif "è‹±è¯­ï¼š" in line or "è‹±æ–‡ï¼š" in line:
            examples.append(line)

    return memory_story, examples, related


def build_word_post_from_sections(word: str, å•è¯å¡: str, æ­£æ–‡: str) -> WordPost:
    """
    å°† Parser æ‹†å‡ºçš„å­—æ®µè½¬ä¸ºå”¯ä¸€ç»“æ„ WordPostã€‚
    ä¸å†³å®šæ ¼å¼ï¼Œåªåšå­—æ®µæå–ä¸æ˜ å°„ã€‚
    """
    definitions = (å•è¯å¡ or "").strip()
    memory_story, examples, related = _parse_body_into_word_post(æ­£æ–‡ or "")
    return {
        "word": word,
        "definitions": definitions,
        "memory_story": memory_story,
        "examples": examples,
        "related": related,
    }


class WordLearningParser(StructuredPostParser):
    """å•è¯å­¦ä¹ å¸–è§£æå™¨ï¼ˆç»§æ‰¿åŸºç±»ï¼‰"""
    
    sections = ("ã€æ ‡é¢˜ã€‘", "ã€å•è¯å¡ã€‘", "ã€é…å›¾å»ºè®®ã€‘", "ã€æ­£æ–‡ã€‘", "ã€æ ‡ç­¾ã€‘", "ã€metaã€‘")
    
    def _post_process(self, sections: Dict[str, str], word: str) -> Dict[str, Any]:
        """å°†è§£æå‡ºçš„æ®µè½è½¬æ¢ä¸ºä¸šåŠ¡æ ¼å¼"""
        title = sections.get("ã€æ ‡é¢˜ã€‘", "").strip() or f"ğŸ“š ä»Šå¤©å­¦å•è¯ï¼š{word}"
        å•è¯å¡ = sections.get("ã€å•è¯å¡ã€‘", "").strip()
        é…å›¾å»ºè®® = sections.get("ã€é…å›¾å»ºè®®ã€‘", "").strip()
        æ­£æ–‡ = sections.get("ã€æ­£æ–‡ã€‘", "").strip()
        æ ‡ç­¾_raw = sections.get("ã€æ ‡ç­¾ã€‘", "").strip()
        meta_raw = sections.get("ã€metaã€‘", "").strip()
        
        tags = self.extract_tags(æ ‡ç­¾_raw)
        if not tags:
            tags = ["è‹±è¯­å­¦ä¹ ", "è®°å•è¯", "è‹±è¯­è¯æ±‡", "å­¦ä¹ æ‰“å¡", "è‹±è¯­å¹²è´§"]
        
        meta = self.extract_meta(meta_raw)
        
        # Parser åªæ‹†å­—æ®µï¼Œä¸æ‹¼æ–‡æ¡ˆï¼›content ç”± main ç»å”¯ä¸€ Renderer ç”Ÿæˆ
        return {
            "word": word,
            "title": title,
            "å•è¯å¡": å•è¯å¡,
            "æ­£æ–‡": æ­£æ–‡,
            "tags": tags[:8],
            "image_suggestion": é…å›¾å»ºè®® or None,
            "meta": meta,
        }


class ContentGenerator:
    """AIæ–‡æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.word_parser = WordLearningParser()
    
    def generate_word_post(self, word: str, level: str = "CET-6") -> str:
        """
        ä½¿ç”¨ç‹¬ç«‹ Prompt æ¨¡æ¿ç”Ÿæˆè‹±è¯­å•è¯å­¦ä¹ å¸–æ–‡æ¡ˆã€‚
        å†…éƒ¨è°ƒç”¨ llm_client.generate_textï¼Œä¸å†™æ­» Promptã€‚

        Args:
            word: è¦å­¦ä¹ çš„è‹±è¯­å•è¯
            level: éš¾åº¦æ°´å¹³ï¼Œé»˜è®¤ "CET-6"

        Returns:
            LLM ç”Ÿæˆçš„æ–‡æ¡ˆåŸæ–‡ï¼ˆå­—ç¬¦ä¸²ï¼‰
        """
        prompt = build_word_learning_prompt(word=word, level=level)
        return generate_text(prompt)

    def parse_structured_word_post(self, text: str, word: str) -> dict:
        """
        è§£æã€Œå…­æ®µå¼ã€ç»“æ„åŒ–è¾“å‡ºï¼ˆå«ã€metaã€‘ï¼‰ï¼Œåªæ‹†å­—æ®µï¼Œä¸æ‹¼æ–‡æ¡ˆã€‚
        Returns å« word, title, å•è¯å¡, æ­£æ–‡, tags, image_suggestion, metaï¼›
        content ç”±è°ƒç”¨æ–¹é€šè¿‡å”¯ä¸€ Renderer ç”Ÿæˆã€‚
        """
        return self.word_parser.parse(text, word=word)

    def render_word_post_content(self, parsed: Dict[str, Any]) -> str:
        """
        å”¯ä¸€å‡ºå£ï¼šå°†è§£æç»“æœè½¬ä¸º WordPost åäº¤ç»™ Renderer ç”Ÿæˆæœ€ç»ˆæ­£æ–‡ã€‚
        main ç¦æ­¢æ‹¼æ–‡æ¡ˆï¼Œåªå…è®¸è°ƒç”¨æ­¤æ–¹æ³•ã€‚
        """
        word_post = build_word_post_from_sections(
            word=parsed["word"],
            å•è¯å¡=parsed.get("å•è¯å¡") or "",
            æ­£æ–‡=parsed.get("æ­£æ–‡") or "",
        )
        return render_xhs_word_post(word_post)

    def get_words_for_level(self, level: str) -> List[str]:
        """
        æŒ‰ level è¯»å–å•è¯åº“æ–‡ä»¶ï¼Œè¿”å›å•è¯åˆ—è¡¨ï¼ˆä¸€è¡Œä¸€ä¸ªï¼Œå»ç©ºã€å»é¦–å°¾ç©ºç™½ï¼‰ã€‚

        Args:
            level: éš¾åº¦çº§åˆ«ï¼Œå¦‚ "CET-4"ã€"CET-6"ã€"è€ƒç ”"ã€"CET4"ã€"CET6" "cet-4"ã€"cet-6"ã€"cet4"ã€"cet6"

        Returns:
            è¯¥çº§åˆ«å•è¯åº“ä¸­çš„å…¨éƒ¨å•è¯åˆ—è¡¨

        Raises:
            ValueError: ä¸æ”¯æŒçš„ level
            FileNotFoundError: å¯¹åº”å•è¯åº“æ–‡ä»¶ä¸å­˜åœ¨
        """
        path_str = LEVEL_WORD_FILES.get(level)
        if not path_str:
            raise ValueError(f"ä¸æ”¯æŒçš„ levelï¼ˆå•è¯åº“æœªé…ç½®ï¼‰: {level}ï¼Œå¯é€‰: {list(LEVEL_WORD_FILES.keys())}")
        path = Path(path_str)
        if not path.exists():
            raise FileNotFoundError(f"å•è¯åº“æ–‡ä»¶ä¸å­˜åœ¨: {path.absolute()}")
        with path.open("r", encoding="utf-8") as f:
            words = [line.strip() for line in f if line.strip()]
        return words

    def pick_unused_word(
        self,
        level: str,
        has_posted: Callable[[str, str, str], bool],
    ) -> str:
        """
        ä»æŒ‡å®š level çš„å•è¯åº“ä¸­ï¼Œé€‰å‡ºä¸€ä¸ªã€Œæœªåœ¨ posts è¡¨ä¸­å‡ºç°è¿‡ï¼ˆword + level + PROMPT_VERSIONï¼‰ã€çš„å•è¯ã€‚

        Args:
            level: éš¾åº¦çº§åˆ«
            has_posted: åˆ¤æ–­æ˜¯å¦å·²å‘è¿‡ï¼Œç­¾å (word, level, prompt_version) -> bool

        Returns:
            ä¸€ä¸ªæœªå‘è¿‡çš„å•è¯

        Raises:
            AllWordsUsedError: è¯¥ level ä¸‹æ‰€æœ‰å•è¯å‡å·²å‘è¿‡
        """
        words = self.get_words_for_level(level)
        unused = [
            w for w in words
            if not has_posted(w.strip(), level, PROMPT_VERSION)
        ]
        if not unused:
            raise AllWordsUsedError(level)
        return random.choice(unused).strip()

    def generate_word_content(
        self,
        word: Optional[str] = None,
        theme: Optional[str] = None,
        level: Optional[str] = None,
    ) -> dict:
        """
        ç”Ÿæˆè®°å•è¯æ–‡æ¡ˆã€‚å”¯ä¸€ Prompt æ¥æºï¼šbuild_word_learning_promptã€‚
        ä¸æ‹¼å†™ Prompt å­—ç¬¦ä¸²ï¼›ä¸ theme==word åŒåè®®ã€åŒè§£æã€åŒ Rendererã€‚
        """
        level = level or (theme if theme and theme in LEVEL_WORD_FILES else "CET-4")
        if not word or not word.strip():
            word = self._get_random_word(level)
        else:
            word = word.strip()
        prompt = build_word_learning_prompt(word=word, level=level)
        text = generate_text(prompt)
        content_data = self.parse_structured_word_post(text, word)
        content_data["content"] = self.render_word_post_content(content_data)
        return content_data
    
    def _get_random_word(self, level: str) -> str:
        """ä»æŒ‡å®š level å•è¯åº“ä¸­éšæœºå–ä¸€è¯ï¼ˆä¸æŸ¥ postsï¼Œç”¨äº generate_word_contentï¼‰ã€‚"""
        words = self.get_words_for_level(level)
        return random.choice(words).strip()
